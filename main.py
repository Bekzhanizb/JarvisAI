import os
import json
import wave
import tempfile
import numpy as np
import sounddevice as sd
from vosk import Model, KaldiRecognizer
from scipy.signal import resample
from gtts import gTTS
from openai import OpenAI
from dotenv import load_dotenv
import time
import psycopg2

# ====== Parameters ======
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

DB_CONFIG = {
    "dbname": "shyraq_db",
    "user": "postgres",
    "password": "1234",
    "host": "localhost",
    "port": 5432
}

# ====== Initializing ======
client = OpenAI(api_key=OPENAI_API_KEY)
conn = psycopg2.connect(**DB_CONFIG)
cursor = conn.cursor()

cursor.execute("""
CREATE TABLE IF NOT EXISTS dialog_history (
    id SERIAL PRIMARY KEY,
    role TEXT,
    content TEXT
);
""")
conn.commit()

# ====== Kazakh Speech Model (Vosk) ======
MODEL_PATH = "vosk-model-kz-0.42"
print("üîÑ Vosk –º–æ–¥–µ–ª—ñ–Ω –∂“Ø–∫—Ç–µ—É...")
model = Model(MODEL_PATH)
print("‚úÖ “ö–∞–∑–∞“õ —Ç—ñ–ª—ñ –º–æ–¥–µ–ª—ñ —Å”ô—Ç—Ç—ñ –∂“Ø–∫—Ç–µ–ª–¥—ñ!")

# ====== DB functions ======
def save_message(role, content):
    cursor.execute("INSERT INTO dialog_history (role, content) VALUES (%s, %s)", (role, content))
    conn.commit()

# ===== Session memory =====

session_history = []

# ====== Auto Select Microphone ======

def auto_select_microphone():
    devices = sd.query_devices()
    for i, dev in enumerate(devices):
        if dev['max_input_channels'] > 0:
            sd.default.device = i
            print(f"–ú–∏–∫—Ä–æ—Ñ–æ–Ω —Ç–∞“£–¥–∞–ª–¥—ã: {dev['name']}")
            return
    raise Exception("–ú–∏–∫—Ä–æ—Ñ–æ–Ω —Ç–∞–±—ã–ª–º–∞–¥—ã!")

# ===== Valid Sample Rate =====

def get_valid_sample_rate():
    device = sd.query_devices(sd.default.device, 'input')
    rates = [16000, 32000, 44100, 48000]

    for r in rates:
        try:
            sd.check_input_settings(samplerate=r, channels=1)
            print(f"üéö –ú–∏–∫—Ä–æ—Ñ–æ–Ω {r} Hz –∂–∏—ñ–ª—ñ–≥—ñ–Ω “õ–æ–ª–¥–∞–π–¥—ã")
            return r
        except:
            continue

    raise Exception("‚ùå –ú–∏–∫—Ä–æ—Ñ–æ–Ω “Ø—à—ñ–Ω “õ–æ–ª–∞–π–ª—ã sample rate —Ç–∞–±—ã–ª–º–∞–¥—ã.")


# ===== Main values =====

auto_select_microphone()  #Auto selecting Microphone

MIC_RATE = get_valid_sample_rate() #Microphone Rate(Hz)

# ====== Recording ======

def record_voice_auto(fs=MIC_RATE,
                      silence_threshold=500,
                      silence_limit=1.2,
                      max_wait_before_speech=10):
    print("üéô –ö“Ø—Ç—É —Ä–µ–∂–∏–º—ñ: –¥–∞—É—ã—Å—Ç—ã 10 —Å–µ–∫—É–Ω–¥ —ñ—à—ñ–Ω–¥–µ –∞–Ω—ã“õ—Ç–∞—É...")

    recording = []
    chunk_duration = 0.1
    chunk_size = int(fs * chunk_duration)

    silent_time = 0
    active_speech_detected = False
    start_wait = time.time()

    while True:
        chunk = sd.rec(chunk_size, samplerate=fs, channels=1, dtype=np.int16)
        sd.wait()
        amplitude = abs(chunk).mean()

        if not active_speech_detected:
            if amplitude > silence_threshold:
                print("üé§ –î–∞—É—ãc –∞–Ω—ã“õ—Ç–∞–ª–¥—ã!")
                active_speech_detected = True
                recording.append(chunk)
            else:
                if time.time() - start_wait >= max_wait_before_speech:
                    print("‚è≥ 10 —Å–µ–∫—É–Ω–¥ —ñ—à—ñ–Ω–¥–µ –¥–∞—É—ã—Å –±–æ–ª–º–∞–¥—ã. Jarvis ”©—à—ñ—Ä—ñ–ª–µ–¥—ñ.")
                    return None
                continue
        else:
            recording.append(chunk)

            if amplitude < silence_threshold:
                silent_time += chunk_duration
            else:
                silent_time = 0

            if silent_time >= silence_limit:
                print("üîá –¢—ã–Ω—ã—à—Ç—ã“õ –∞–Ω—ã“õ—Ç–∞–ª–¥—ã ‚Äî –∂–∞–∑—É —Ç–æ“õ—Ç–∞–¥—ã.")
                break

    audio_data = np.concatenate(recording, axis=0)

    # --- Resampling 16kHz ---
    if fs != 16000:
        target_length = int(len(audio_data) * 16000 / fs)
        audio_data = resample(audio_data, target_length).astype(np.int16)
        fs = 16000

    filename = tempfile.mktemp(prefix="voice_", suffix=".wav")
    with wave.open(filename, "wb") as f:
        f.setnchannels(1)
        f.setsampwidth(2)
        f.setframerate(fs)
        f.writeframes(audio_data.tobytes())

    return filename


# ====== Transcribing (Vosk) ======
def transcribe_audio_vosk(filename):
    wf = wave.open(filename, "rb")
    rec = KaldiRecognizer(model, wf.getframerate())

    full_text = ""

    while True:
        data = wf.readframes(8000)
        if len(data) == 0:
            break

        if rec.AcceptWaveform(data):
            result = json.loads(rec.Result())
            text = result.get("text", "")
            full_text += text + " "

    # Final result
    final = json.loads(rec.FinalResult())
    final_text = final.get("text", "")

    result_text = (full_text + " " + final_text).strip()
    return result_text



# ====== Voice the text ======

def speak_kz(text):
    print("üîä Jarvis —Å”©–π–ª–µ–ø –∂–∞—Ç—ã—Ä (Kazakh Male Adaptive Voice)...")
    speech_path = tempfile.mktemp(suffix=".wav")

    with client.audio.speech.with_streaming_response.create(
        model="gpt-4o-mini-tts",
        voice="onyx",  # Male Basic Voice
        input=text,
        extra_body={
            "format": "wav",

            # Adaptive Voice Parameters
            "speed": 1.0,            # the pace of speech will adjust itself.
            "pitch": "auto",            # the pitch of the voice adapts to the context
            "emotion": "auto",          # emotion depends on the text
            "intonation": "auto",       # automatic intonation
            "natural_pauses": True,     # natural pauses
            "emphasis": "balanced",     # a pleasant and soft accent
            "clarity": "high"           # voice clarity
        }
    ) as response:
        response.stream_to_file(speech_path)

    os.system(f"mpg123 {speech_path}")




# ====== Jarvis replying ======

def chat_with_jarvis(prompt):

    session_history.append({"role": "user", "content": prompt}) #Save user message in session

    save_message("user", prompt) #Save message to PostgreSQL

    messages = [{"role": "system",
                 "content": "–°–µ–Ω Jarvis –µ—Å—ñ–º–¥—ñ “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ —Å”©–π–ª–µ–π—Ç—ñ–Ω –∂–∞—Å–∞–Ω–¥—ã –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—ñ—Å—ñ“£. –ü–∞–π–¥–∞–ª–∞–Ω—É—à—ã“ì–∞ —Ç–µ–∫ “õ–∞–∑–∞“õ—à–∞ –∂–∞—É–∞–ø –±–µ—Ä. –ö–æ–Ω—Ç–µ–∫—Å—Ç—Ç—ñ –æ—Å—ã —Å–µ–∞–Ω—Å—Ç–∞ —Å–∞“õ—Ç–∞—É."}
                ] + session_history  #Build messages for the model

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages
    )

    answer = response.choices[0].message.content

    session_history.append({"role": "assistant", "content": answer})
    save_message("assistant", answer) #Save assistant reply

    return answer

# ====== Main loop ======
print("ü§ñ Jarvis v0.2 (“ö–∞–∑–∞“õ—à–∞) —ñ—Å–∫–µ “õ–æ—Å—ã–ª–¥—ã!")
print("–ê–π—Ç“õ—ã“£—ã–∑ –∫–µ–ª–≥–µ–Ω–¥—ñ –∞–π—Ç—ã“£—ã–∑ (–Ω–µ–º–µ—Å–µ '—à—ã“ì—É' –¥–µ–ø –∞—è“õ—Ç–∞“£—ã–∑).")

while True:
    filename = record_voice_auto()

    if filename is None:
        break  # auto-exit

    text = transcribe_audio_vosk(filename)

    if not text:
        print("–°”©–∑ —Ç–∞–Ω—ã–ª–º–∞–¥—ã, “õ–∞–π—Ç–∞–¥–∞–Ω –∞–π—Ç—ã“£—ã–∑.")
        continue

    if text.lower() in ["—à—ã“ì—É", "exit", "stop"]:
        print("Jarvis: –ö”©—Ä—ñ—Å–∫–µ–Ω—à–µ —Å–∞—É –±–æ–ª! üëã")
        break

    print("–°—ñ–∑:", text)
    reply = chat_with_jarvis(text)
    print("ü§ñ Jarvis:", reply)
    speak_kz(reply)

